{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python395jvsc74a57bd099895b1fb5777b62bbca485c28947babc49174d82e5a9d257e359cdc2b435eee",
   "display_name": "Python 3.9.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 1. Business understanding\n",
    "\n",
    "Assessing payment capacity is one of the most important issues when financial institutions need to assign credit limits. Although it might seem trivial, in some cases the information is no available, and due to the informality of some latinamercian economies (Colombia for this study case), it is important to build statistical models that can estimate the income of the customers. \n",
    "\n",
    "## What data do we have?\n",
    "\n",
    "We are going to be using the data from the National Administrative Department of Statistics of Colombia [DANE] (https://www.dane.gov.co/). The database is the result of a survey conducted to more than 25k households in three major cities in Colombia for 2018. \n",
    "\n",
    "All the data and metadata can be found in [this link] (http://microdatos.dane.gov.co/index.php/catalog/626/). The data has 331 variables including spending behaviours and financial burden of the households.\n",
    "\n",
    "## What question do we want to answer?\n",
    "\n",
    "1. Can the income be modeled after the spending patterns of the household?\n",
    "2. Can the income be modeled after the financial burden of the household?\n",
    "3. Is there a possible way financial institutions can include this informetion in theor models?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 2. Data Understanding"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "##  Environment and data\n",
    "\n",
    "First af all, lets load all needed packages and teh data we will be working with."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Administrador\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3169: DtypeWarning: Columns (48,50,51,52,53,54,55,56,57,58,59,60,61,62,63,90,108,117,125,134,136,137,138,139,140,141,142,143,144,145,146,147,280) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 62838 entries, 0 to 62837\n",
      "Columns: 331 entries, SECUENCIA_P to P3045\n",
      "dtypes: int64(29), object(302)\n",
      "memory usage: 158.7+ MB\n",
      "None\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   SECUENCIA_P  ORDEN  DIRECTORIO             FEX_C  INGRESO_COMPLETO  P6050  \\\n",
       "0            1      1     4513825  100,123276403857                 1      1   \n",
       "1            1      2     4513825  100,123276403857                 1      2   \n",
       "2            1      3     4513825  100,123276403857                 1      5   \n",
       "3            1      1     4513826  108,829812109572                 1      1   \n",
       "4            1      1     4513827  110,412858809884                 1      1   \n",
       "\n",
       "   P10 INGTOTOB  DEPARTAMENTO  MUNICIPIO  ...  P3040  P3040_A P3040_B P3040_C  \\\n",
       "0  602  1800000             5          1  ...               0       0           \n",
       "1  407  1500000             5          1  ...               0                   \n",
       "2  305  2000000             5          1  ...               0                   \n",
       "3  304  2000000             5          1  ...                                   \n",
       "4  301   500000             5          1  ...                                   \n",
       "\n",
       "  P3040_D P3040_E P3040_F P3043 P3044 P3045  \n",
       "0                             4     1     1  \n",
       "1                             2     1     3  \n",
       "2               0             1     0     2  \n",
       "3                                   2     3  \n",
       "4               0                   2     2  \n",
       "\n",
       "[5 rows x 331 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SECUENCIA_P</th>\n      <th>ORDEN</th>\n      <th>DIRECTORIO</th>\n      <th>FEX_C</th>\n      <th>INGRESO_COMPLETO</th>\n      <th>P6050</th>\n      <th>P10</th>\n      <th>INGTOTOB</th>\n      <th>DEPARTAMENTO</th>\n      <th>MUNICIPIO</th>\n      <th>...</th>\n      <th>P3040</th>\n      <th>P3040_A</th>\n      <th>P3040_B</th>\n      <th>P3040_C</th>\n      <th>P3040_D</th>\n      <th>P3040_E</th>\n      <th>P3040_F</th>\n      <th>P3043</th>\n      <th>P3044</th>\n      <th>P3045</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>4513825</td>\n      <td>100,123276403857</td>\n      <td>1</td>\n      <td>1</td>\n      <td>602</td>\n      <td>1800000</td>\n      <td>5</td>\n      <td>1</td>\n      <td>...</td>\n      <td></td>\n      <td>0</td>\n      <td>0</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2</td>\n      <td>4513825</td>\n      <td>100,123276403857</td>\n      <td>1</td>\n      <td>2</td>\n      <td>407</td>\n      <td>1500000</td>\n      <td>5</td>\n      <td>1</td>\n      <td>...</td>\n      <td></td>\n      <td>0</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>2</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>3</td>\n      <td>4513825</td>\n      <td>100,123276403857</td>\n      <td>1</td>\n      <td>5</td>\n      <td>305</td>\n      <td>2000000</td>\n      <td>5</td>\n      <td>1</td>\n      <td>...</td>\n      <td></td>\n      <td>0</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>0</td>\n      <td></td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1</td>\n      <td>4513826</td>\n      <td>108,829812109572</td>\n      <td>1</td>\n      <td>1</td>\n      <td>304</td>\n      <td>2000000</td>\n      <td>5</td>\n      <td>1</td>\n      <td>...</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>2</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>1</td>\n      <td>4513827</td>\n      <td>110,412858809884</td>\n      <td>1</td>\n      <td>1</td>\n      <td>301</td>\n      <td>500000</td>\n      <td>5</td>\n      <td>1</td>\n      <td>...</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>0</td>\n      <td></td>\n      <td></td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 331 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "df = pd.read_csv('./IEFIC_2018.csv', sep=';')\n",
    "print(df.info())\n",
    "df.head()"
   ]
  },
  {
   "source": [
    "Reading the database documentation it can be found that there are some household that do not report the total income, and that have one entry for every member of the house (including children). For this reason we are going to keep only the household head and only the households that report total income."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   SECUENCIA_P  ORDEN  DIRECTORIO             FEX_C  INGRESO_COMPLETO  P6050  \\\n",
       "0            1      1     4513825  100,123276403857                 1      1   \n",
       "1            1      1     4513826               NaN                 1      1   \n",
       "2            1      1     4513827               NaN                 1      1   \n",
       "3            1      1     4513829               NaN                 1      1   \n",
       "4            1      1     4513831               NaN                 1      1   \n",
       "\n",
       "   P10 INGTOTOB  DEPARTAMENTO  MUNICIPIO  ...  P3040  P3040_A P3040_B P3040_C  \\\n",
       "0  602  1800000             5          1  ...    NaN        0       0     NaN   \n",
       "1  304  2000000             5          1  ...    NaN      NaN     NaN     NaN   \n",
       "2  301   500000             5          1  ...    NaN      NaN     NaN     NaN   \n",
       "3  607  3125000             5          1  ...    NaN        0     NaN     NaN   \n",
       "4  605  6109000             5          1  ...    NaN        0       0     NaN   \n",
       "\n",
       "  P3040_D P3040_E P3040_F P3043 P3044 P3045  \n",
       "0     NaN     NaN     NaN     4     1     1  \n",
       "1     NaN     NaN     NaN   NaN     2     3  \n",
       "2     NaN       0     NaN   NaN     2     2  \n",
       "3     NaN       2     NaN     6     0     1  \n",
       "4     NaN       0     NaN     2     2     6  \n",
       "\n",
       "[5 rows x 331 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SECUENCIA_P</th>\n      <th>ORDEN</th>\n      <th>DIRECTORIO</th>\n      <th>FEX_C</th>\n      <th>INGRESO_COMPLETO</th>\n      <th>P6050</th>\n      <th>P10</th>\n      <th>INGTOTOB</th>\n      <th>DEPARTAMENTO</th>\n      <th>MUNICIPIO</th>\n      <th>...</th>\n      <th>P3040</th>\n      <th>P3040_A</th>\n      <th>P3040_B</th>\n      <th>P3040_C</th>\n      <th>P3040_D</th>\n      <th>P3040_E</th>\n      <th>P3040_F</th>\n      <th>P3043</th>\n      <th>P3044</th>\n      <th>P3045</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>4513825</td>\n      <td>100,123276403857</td>\n      <td>1</td>\n      <td>1</td>\n      <td>602</td>\n      <td>1800000</td>\n      <td>5</td>\n      <td>1</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>4513826</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>1</td>\n      <td>304</td>\n      <td>2000000</td>\n      <td>5</td>\n      <td>1</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>1</td>\n      <td>4513827</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>1</td>\n      <td>301</td>\n      <td>500000</td>\n      <td>5</td>\n      <td>1</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1</td>\n      <td>4513829</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>1</td>\n      <td>607</td>\n      <td>3125000</td>\n      <td>5</td>\n      <td>1</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>6</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>1</td>\n      <td>4513831</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>1</td>\n      <td>605</td>\n      <td>6109000</td>\n      <td>5</td>\n      <td>1</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>2</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 331 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "df=df[df['INGRESO_COMPLETO']==1]\n",
    "df=df[df['P6050']==1]\n",
    "df=df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "df=df.replace(98, np.nan, regex=True)\n",
    "df=df.replace('98', np.nan, regex=True)\n",
    "df=df.replace(99, np.nan, regex=True)\n",
    "df=df.replace('99', np.nan, regex=True)\n",
    "df=df[df['INGTOTOB'].notna()]\n",
    "df=df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "source": [
    "After reviewing the documentation the following variables were selected. There are basicaly 3 types of variables. Spenditure, debts and investments."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(21033, 78)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   edu_level   income  house_owner house_value   mortage mortage_balance  \\\n",
       "0        602  1800000            1    80000000       NaN             NaN   \n",
       "1        304  2000000            1         NaN       NaN             NaN   \n",
       "2        301   500000            2         NaN       NaN             NaN   \n",
       "3        607  3125000            1   150000000  35000000             NaN   \n",
       "4        605  6109000            1   140000000  22000000             NaN   \n",
       "\n",
       "  spent_edu spent_food spent_clothes spent_water  ... friend_loan_payment  \\\n",
       "0         0     400000           NaN       16000  ...                 NaN   \n",
       "1         0     700000           NaN       40000  ...                 NaN   \n",
       "2         0     800000           NaN       30000  ...                 NaN   \n",
       "3         0     600000         50000       17400  ...                 NaN   \n",
       "4   9000000    1000000        150000       17400  ...                 NaN   \n",
       "\n",
       "  friend_loan_balance stocks stocks_value funds funds_value tdc tdc_value  \\\n",
       "0                 NaN      2          NaN     2         NaN   2       NaN   \n",
       "1                 NaN      2          NaN     2         NaN   2       NaN   \n",
       "2                 NaN      2          NaN     2         NaN   2       NaN   \n",
       "3                 NaN      2          NaN     2         NaN   1       NaN   \n",
       "4                 NaN      2          NaN     2         NaN   2       NaN   \n",
       "\n",
       "  savs_acc savs_acc_value  \n",
       "0        2            NaN  \n",
       "1        2            NaN  \n",
       "2        2            NaN  \n",
       "3        2            NaN  \n",
       "4        2            NaN  \n",
       "\n",
       "[5 rows x 78 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>edu_level</th>\n      <th>income</th>\n      <th>house_owner</th>\n      <th>house_value</th>\n      <th>mortage</th>\n      <th>mortage_balance</th>\n      <th>spent_edu</th>\n      <th>spent_food</th>\n      <th>spent_clothes</th>\n      <th>spent_water</th>\n      <th>...</th>\n      <th>friend_loan_payment</th>\n      <th>friend_loan_balance</th>\n      <th>stocks</th>\n      <th>stocks_value</th>\n      <th>funds</th>\n      <th>funds_value</th>\n      <th>tdc</th>\n      <th>tdc_value</th>\n      <th>savs_acc</th>\n      <th>savs_acc_value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>602</td>\n      <td>1800000</td>\n      <td>1</td>\n      <td>80000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>400000</td>\n      <td>NaN</td>\n      <td>16000</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>304</td>\n      <td>2000000</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>700000</td>\n      <td>NaN</td>\n      <td>40000</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>301</td>\n      <td>500000</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>800000</td>\n      <td>NaN</td>\n      <td>30000</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>607</td>\n      <td>3125000</td>\n      <td>1</td>\n      <td>150000000</td>\n      <td>35000000</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>600000</td>\n      <td>50000</td>\n      <td>17400</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>605</td>\n      <td>6109000</td>\n      <td>1</td>\n      <td>140000000</td>\n      <td>22000000</td>\n      <td>NaN</td>\n      <td>9000000</td>\n      <td>1000000</td>\n      <td>150000</td>\n      <td>17400</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 78 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "keep=['P10','INGTOTOB','P2439','P2461','P2168','P2471_4','P2477','P2478_1','P2478_2','P2478_3','P2478_4','P2478_5','P2478_6','P2478_7','P2478_8','P2478_9','P2478_10','P2478_11','P2478_12','P2481_1','P2481_2','P2481_3','P2481_4','P2481_5','P2481_6','P2481_7','P2481_8','P2481_9','P2481_10','P2481_11','P2481_12','P2481_13','P2481_14','P2481_15','P2481_16','P2481_17','P2481_18','P2982','P2983','P2985','P2487','P2502','P2503','P2504','P342','P2540','P2542_3','P2542_4','P2545','P2548','P2560_3','P2560_4','P2602','P2623_3','P2623_4','P2633','P2637_3','P2637_4','P2692','P2772_3','P2772_4','P2695','P2736_3','P2736_4','P2734','P2696_3','P2696_4','P2771','P2693_3','P2693_4','P2819','P2869','P622','P1136','P1239','P1421','P2584','P2962']\n",
    "\n",
    "df=df[keep]\n",
    "\n",
    "names=['edu_level','income','house_owner','house_value','mortage','mortage_balance','spent_edu','spent_food','spent_clothes','spent_water','spent_energy','spent_gas','spent_cell','spent_housekeep','spent_leisure','spent_health','spent_internet','spent_transport','spent_pension','extra_house','extra_home','extra_jewelry','extra_art','extra_rent','extra_vacations','extra_retirement','extra_emergency','extra_future','extra_edu','extra_debts','extra_health','extra_children','extra_wedding','extra_invest','extra_heritage','extra_remodeling','extra_savings','small_business','small_business_value','real_estate','real_estate_value','vehicles','vehicles_value','machinery','machinery_value','credit_cards','credit_cards_payment','credit_cards_balance','credit_cards_term','pawnshop','pawnshop_payment','pawnshop_balance','loans','loans_payment','loans_balance','shark','shark_payment','shark_balance','shop','shop_payment','shop_balance','union','union_payment','union_balance','edu_loan','edu_loan_payment','edu_loan_balance','friend_loan','friend_loan_payment','friend_loan_balance','stocks','stocks_value','funds','funds_value','tdc','tdc_value','savs_acc','savs_acc_value']\n",
    "\n",
    "df.columns=names\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "source": [
    "Now we are going to check for nulls and keep the variables that makes sense."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                               column_name  percent_missing\nedu_level                        edu_level            0.000\nincome                              income            0.000\nhouse_owner                    house_owner            0.000\nreal_estate                    real_estate            0.024\nvehicles                          vehicles            0.024\nmachinery                        machinery            0.024\nsmall_business              small_business            0.024\nspent_pension                spent_pension            0.143\ncredit_cards                  credit_cards            0.238\nfriend_loan                    friend_loan            0.242\ntdc                                    tdc            0.242\nfunds                                funds            0.242\nstocks                              stocks            0.242\npawnshop                          pawnshop            0.242\nloans                                loans            0.242\nsavs_acc                          savs_acc            0.242\nshark                                shark            0.242\nedu_loan                          edu_loan            0.242\nunion                                union            0.242\nshop                                  shop            0.242\nspent_housekeep            spent_housekeep            0.285\nspent_transport            spent_transport            0.723\nspent_health                  spent_health            0.846\nspent_food                      spent_food            0.865\nspent_cell                      spent_cell            1.189\nspent_edu                        spent_edu            1.735\nspent_leisure                spent_leisure            2.016\nspent_internet              spent_internet            2.230\nspent_gas                        spent_gas            4.507\nspent_water                    spent_water            5.805\nspent_energy                  spent_energy            5.886\nspent_clothes                spent_clothes           13.108\nvehicles_value              vehicles_value           76.461\nsavs_acc_value              savs_acc_value           79.694\nhouse_value                    house_value           81.634\ncredit_cards_term        credit_cards_term           87.149\ncredit_cards_payment  credit_cards_payment           87.772\nextra_emergency            extra_emergency           88.794\ncredit_cards_balance  credit_cards_balance           89.079\nextra_house                    extra_house           89.165\nextra_savings                extra_savings           90.472\nloans_payment                loans_payment           93.144\nmortage                            mortage           93.719\nloans_balance                loans_balance           94.271\nshop_payment                  shop_payment           95.569\nshop_balance                  shop_balance           96.244\nfriend_loan_balance    friend_loan_balance           96.990\nfriend_loan_payment    friend_loan_payment           97.095\nextra_vacations            extra_vacations           97.357\nunion_payment                union_payment           97.609\nsmall_business_value  small_business_value           97.922\nmortage_balance            mortage_balance           97.975\nunion_balance                union_balance           98.146\nshark_payment                shark_payment           98.512\nextra_retirement          extra_retirement           98.531\nshark_balance                shark_balance           98.788\nreal_estate_value        real_estate_value           99.087\ntdc_value                        tdc_value           99.396\nextra_rent                      extra_rent           99.429\nmachinery_value            machinery_value           99.463\nextra_edu                        extra_edu           99.496\nextra_invest                  extra_invest           99.610\nstocks_value                  stocks_value           99.629\nfunds_value                    funds_value           99.639\nedu_loan_payment          edu_loan_payment           99.696\nedu_loan_balance          edu_loan_balance           99.700\nextra_future                  extra_future           99.705\nextra_home                      extra_home           99.724\nextra_remodeling          extra_remodeling           99.767\npawnshop_payment          pawnshop_payment           99.805\npawnshop_balance          pawnshop_balance           99.838\nextra_debts                    extra_debts           99.886\nextra_health                  extra_health           99.900\nextra_heritage              extra_heritage           99.948\nextra_jewelry                extra_jewelry           99.967\nextra_children              extra_children           99.976\nextra_art                        extra_art           99.986\nextra_wedding                extra_wedding           99.990\n"
     ]
    }
   ],
   "source": [
    "percent_missing = df.isnull().sum() * 100 / len(df)\n",
    "missing_value_df = pd.DataFrame({'column_name': df.columns,\n",
    "                                 'percent_missing': percent_missing})\n",
    "missing_value_df.sort_values('percent_missing', inplace=True)\n",
    "\n",
    "with pd.option_context('display.max_rows', None,\n",
    "                       'display.max_columns', None,\n",
    "                       'display.precision', 3,\n",
    "                       ):\n",
    "    print(missing_value_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(21033, 35)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    income spent_edu spent_food spent_clothes spent_water spent_energy  \\\n",
       "0  1800000         0     400000           NaN       16000        67000   \n",
       "1  2000000         0     700000           NaN       40000        50000   \n",
       "2   500000         0     800000           NaN       30000        60000   \n",
       "3  3125000         0     600000         50000       17400        30000   \n",
       "4  6109000   9000000    1000000        150000       17400        74000   \n",
       "\n",
       "  spent_gas spent_cell spent_housekeep spent_leisure  ... shop_payment  \\\n",
       "0     13000      10000               0        200000  ...          NaN   \n",
       "1     23000      60000               0        100000  ...          NaN   \n",
       "2     10000      10000               0             0  ...          NaN   \n",
       "3      9000      50000               0             0  ...          NaN   \n",
       "4      5000      60000               0        300000  ...          NaN   \n",
       "\n",
       "  shop_balance union_payment union_balance  edu_loan_payment edu_loan_balance  \\\n",
       "0          NaN           NaN           NaN               NaN              NaN   \n",
       "1          NaN           NaN           NaN               NaN              NaN   \n",
       "2          NaN           NaN           NaN               NaN              NaN   \n",
       "3          NaN           NaN           NaN               NaN              NaN   \n",
       "4          NaN           NaN           NaN               NaN              NaN   \n",
       "\n",
       "  friend_loan_payment friend_loan_balance vehicles_value savs_acc_value  \n",
       "0                 NaN                 NaN       16000000            NaN  \n",
       "1                 NaN                 NaN        1000000            NaN  \n",
       "2                 NaN                 NaN            NaN            NaN  \n",
       "3                 NaN                 NaN            NaN            NaN  \n",
       "4                 NaN                 NaN        7000000            NaN  \n",
       "\n",
       "[5 rows x 35 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>income</th>\n      <th>spent_edu</th>\n      <th>spent_food</th>\n      <th>spent_clothes</th>\n      <th>spent_water</th>\n      <th>spent_energy</th>\n      <th>spent_gas</th>\n      <th>spent_cell</th>\n      <th>spent_housekeep</th>\n      <th>spent_leisure</th>\n      <th>...</th>\n      <th>shop_payment</th>\n      <th>shop_balance</th>\n      <th>union_payment</th>\n      <th>union_balance</th>\n      <th>edu_loan_payment</th>\n      <th>edu_loan_balance</th>\n      <th>friend_loan_payment</th>\n      <th>friend_loan_balance</th>\n      <th>vehicles_value</th>\n      <th>savs_acc_value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1800000</td>\n      <td>0</td>\n      <td>400000</td>\n      <td>NaN</td>\n      <td>16000</td>\n      <td>67000</td>\n      <td>13000</td>\n      <td>10000</td>\n      <td>0</td>\n      <td>200000</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>16000000</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2000000</td>\n      <td>0</td>\n      <td>700000</td>\n      <td>NaN</td>\n      <td>40000</td>\n      <td>50000</td>\n      <td>23000</td>\n      <td>60000</td>\n      <td>0</td>\n      <td>100000</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1000000</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>500000</td>\n      <td>0</td>\n      <td>800000</td>\n      <td>NaN</td>\n      <td>30000</td>\n      <td>60000</td>\n      <td>10000</td>\n      <td>10000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3125000</td>\n      <td>0</td>\n      <td>600000</td>\n      <td>50000</td>\n      <td>17400</td>\n      <td>30000</td>\n      <td>9000</td>\n      <td>50000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6109000</td>\n      <td>9000000</td>\n      <td>1000000</td>\n      <td>150000</td>\n      <td>17400</td>\n      <td>74000</td>\n      <td>5000</td>\n      <td>60000</td>\n      <td>0</td>\n      <td>300000</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>7000000</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 35 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "keep_2=['income','spent_edu','spent_food','spent_clothes','spent_water','spent_energy','spent_gas','spent_cell','spent_housekeep','spent_leisure','spent_health','spent_internet','spent_transport','spent_pension','edu_level','house_value','credit_cards_payment','credit_cards_balance','credit_cards_term','pawnshop_payment','pawnshop_balance','loans_payment','loans_balance','shark_payment','shark_balance','shop_payment','shop_balance','union_payment','union_balance','edu_loan_payment','edu_loan_balance','friend_loan_payment','friend_loan_balance','vehicles_value','savs_acc_value']\n",
    "df=df[keep_2]\n",
    "df=df.reset_index(drop=True)\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "source": [
    "### Lets do some EDA over Listings"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Lets beging diferentiationg numeric and categorical variables"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lis.dtypes"
   ]
  },
  {
   "source": [
    "It seems we have some issues with the two variables 'price' and 'security_deposit'. We have to change the type to float but doing some replacements first."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lis['price'] = df_lis['price'].str.replace('$', '')\n",
    "df_lis['price'] = df_lis['price'].str.replace(',', '')\n",
    "df_lis['price']=df_lis['price'].astype('float')\n",
    "\n",
    "df_lis['host_acceptance_rate'] = df_lis['host_acceptance_rate'].str.replace('%', '')\n",
    "df_lis['host_acceptance_rate']=df_lis['host_acceptance_rate'].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we split numeric and categorical variables in two datasets\n",
    "l_num_vars = df_lis[df_lis.select_dtypes(include=['float', 'int']).columns]\n",
    "l_cat_vars = df_lis[df_lis.select_dtypes(include=['object']).columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets plot some correlations\n",
    "sns.heatmap(l_num_vars.corr())\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "There seem to be some obvoius correlations:\n",
    "\n",
    "1. The scores have the biggest correlation between them.\n",
    "2. Between the number of bedrooms and the number of bed.\n",
    "\n",
    "But there are some that are not that simple:\n",
    "\n",
    "1. It seems that the socre that are more correlate with price are location and cleanliness\n",
    "2. Latitude seems to be more correlated with price that longitude"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_lis=l_num_vars.corr().stack().reset_index()\n",
    "corr_lis.columns = ['var_1','var_2','corr']\n",
    "price_corr=corr_lis[corr_lis['var_1']=='price'].sort_values(by=['corr'])  \n",
    "price_corr=price_corr[price_corr['var_2']!='price']\n",
    "\n",
    "x = price_corr['var_2']\n",
    "y = price_corr['corr']\n",
    "\n",
    "mask1 = y < 0\n",
    "mask2 = y >= 0\n",
    "\n",
    "plt.xticks(rotation = 90)\n",
    "plt.bar(x[mask1], y[mask1], color = 'bisque')\n",
    "plt.bar(x[mask2], y[mask2], color = 'turquoise')\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    " Checking correlations only with 'price' it is clear that the number of bedrooms/beds, therefore the size of the listing, is strongly correlated with the price. Location (latitude and longitude) have a some correlation with price."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Reviews!\n",
    "\n",
    "This dataset has unique id for each reviewer and detailed comments\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev=df_rev.drop(['id','date','reviewer_name'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev.groupby(['listing_id']).count().sort_values(by=['comments'],ascending=False).head()"
   ]
  },
  {
   "source": [
    "Some listing have up to 402 comments!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev.groupby(['reviewer_id']).count().sort_values(by=['comments'],ascending=False).head()"
   ]
  },
  {
   "source": [
    "How much traveling does it takes to visit 37 different listing!\n",
    "\n",
    "This dataset is pretty straight forward. In the following section we are going to try and use some sort of prebuilt sentiment analisys tool so we can clasify the comments and try to predict the price :)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 3. Data Preparation\n",
    "\n",
    "In this section we are going to get our databases ready for modeling, doing some data cleaning, transformation and imputation\n",
    "\n",
    "## Reviews!\n",
    "\n",
    "The following list shows the transformation to be made in each of the columns in this dataset\n",
    "\n",
    "**price:** apply logarithm and Normalization \n",
    "\n",
    "1. **host_since:** Calculate the monts this host hase been related to Airbnb and Normalization\n",
    "2 **host_response_time:** Dummify\n",
    "3. **host_response_rate:** Normalization\n",
    "4. **host_acceptance_rate:** Normalization\n",
    "5. **host_is_superhost:** Dummify\n",
    "6. **host_total_listings_count:** Normalization\n",
    "7. **latitude:** Normalization\n",
    "8. **longitude:** Normalization\n",
    "9. **property_type:** Dummify\n",
    "10. **room_type:** Dummify\n",
    "11. **accommodates:** Normalization\n",
    "12. **bathrooms:** Normalization\n",
    "13. **bedrooms:** Normalization\n",
    "14.**beds:** Normalization\n",
    "15.**price:** Normalization\n",
    "16.**guests_included:** Normalization\n",
    "17. **number_of_reviews:** Normalization\n",
    "18. **review_scores_rating:** Normalization\n",
    "19. **review_scores_accuracy:** Normalization\n",
    "20. **review_scores_cleanliness:** Normalization\n",
    "21. **review_scores_checkin:** Normalization\n",
    "22. **review_scores_communication:** Normalization\n",
    "23. **review_scores_location:** Normalization\n",
    "24. **review_scores_value:** Normalization\n",
    "25. **instant_bookable:** Dummify\n",
    "\n",
    "But first, we are going to perform some type changes an calculations in some variables"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we are calculating the number of months between the most recent host and all the others\n",
    "\n",
    "df_lis.host_since=pd.to_datetime(df_lis.host_since)\n",
    "df_lis.host_since=(max(df_lis.host_since)-df_lis.host_since)\n",
    "df_lis.host_since=(df_lis.host_since/ np.timedelta64(1, 'D')).astype(float)/30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we change the type of the percentage.\n",
    "\n",
    "df_lis.host_response_rate = df_lis.host_response_rate.str.replace('%', '')\n",
    "df_lis.host_response_rate = df_lis.host_response_rate.astype(float)"
   ]
  },
  {
   "source": [
    "Before normalizing an getting the dummy variables for the categories, we are going to apply some prebuilt sentiment analyzer on the reviwes. We are going to use NLTK prebuilt sentyment analyzer."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos(comment):\n",
    "    '''\n",
    "    INPUT\n",
    "    comment - string \n",
    "    OUTPUT\n",
    "    Positive score of Nltk sentimen analyzer.\n",
    "    '''\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    pos=sia.polarity_scores(comment).get('pos')\n",
    "    return pos\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we are going to apply our funtion to every coment, and average the value by listing id\n",
    "\n",
    "df_rev.comments = df_rev.comments.astype('str') \n",
    "df_rev['positive']=df_rev.comments.apply(get_pos)\n",
    "df_rev_mean=df_rev.groupby('listing_id')['positive'].mean().to_frame()"
   ]
  },
  {
   "source": [
    "## Joining Resulting Dataframes\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lis=df_lis.set_index('id')\n",
    "mergedDf = df_lis.merge(df_rev_mean, left_index=True, right_index=True)"
   ]
  },
  {
   "source": [
    "This following function is based on the one showd in the solution of some quizes in Udacitys nanodegree.\n",
    "\n",
    "# Something missing?\n",
    "\n",
    "We are going to handle missin values in three ways:\n",
    "\n",
    "1. Price: this is our lable, so we are going to drop all null values.\n",
    "2. Numeric variables: The numeric values are going to be filled with the mean. \n",
    "3. Categorical variables: In this case the absence of variable in the dummies captures the null values.\n",
    "\n",
    "This is safe for our model, becaouse there woulfd be no considerable variance for the inputed variables.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    '''\n",
    "    INPUT\n",
    "    df - pandas dataframe \n",
    "    \n",
    "    OUTPUT\n",
    "    X - A matrix holding all of the variables you want to consider when predicting the response\n",
    "    y - the corresponding response vector\n",
    "    \n",
    "    This function cleans df using the following steps to produce X and y:\n",
    "    1. Drop all the rows with no salaries\n",
    "    2. Create X as all the columns that are not the Salary column\n",
    "    3. Create y as the Salary column\n",
    "    4. Drop the Salary, Respondent, and the ExpectedSalary columns from X\n",
    "    5. For each numeric variable in X, fill the column with the mean value of the column.\n",
    "    5. For each numeric variable in X, perform min max normalization\n",
    "    6. Create dummy columns for all the categorical variables in X, drop the original columns\n",
    "    '''\n",
    "    # Drop rows with missing price values\n",
    "    df = df.dropna(subset=['price'], axis=0)\n",
    "    y = df['price']\n",
    "    y = np.log(y)\n",
    "    y = (y-min(y))/(max(y)-min(y))\n",
    "\n",
    "\n",
    "    #Drop price columns\n",
    "    df = df.drop(['price'], axis=1)\n",
    "    \n",
    "    # Fill numeric columns with the mean\n",
    "    num_vars = df.select_dtypes(include=['float', 'int']).columns\n",
    "    for col in num_vars:\n",
    "        df[col].fillna((df[col].mean()), inplace=True)\n",
    "        df[col]=(df[col]-min(df[col]))/(max(df[col])-min(df[col]))\n",
    "        \n",
    "    # Dummy the categorical variables\n",
    "    cat_vars = df.select_dtypes(include=['object']).copy().columns\n",
    "    for var in  cat_vars:\n",
    "        # for each cat add dummy var, drop original column\n",
    "        df = pd.concat([df.drop(var, axis=1), pd.get_dummies(df[var], prefix=var, prefix_sep='_', drop_first=True)], axis=1)\n",
    "    \n",
    "    X = df\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedDf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = clean_data(mergedDf)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "source": [
    "Now we have the datasets ready to model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 4. Modeling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "In this section we are going to use a simple random forest to predict the price of the listings. We are going be doing the following steps:\n",
    "\n",
    "1. Split data\n",
    "2. Instantiate model and fit\n",
    "4. Evaluate"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Split data\n",
    "\n",
    "The data will bi splitted 80/20 for train and test\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Features Shape:', X_train.shape)\n",
    "print('Training Labels Shape:', y_train.shape)\n",
    "print('Testing Features Shape:', X_test.shape)\n",
    "print('Testing Labels Shape:', y_test.shape)"
   ]
  },
  {
   "source": [
    "## Instantiate and fit model\n",
    "\n",
    "We are going to use a simple Random forest with 1000 estimators.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 321)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "source": [
    "## Evaluate Model\n",
    "\n",
    "Since we did some transformations to the lable (price) for the error meassure to make sense we have to reverse this transormations to our predictions and test labels.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deform(t_price):\n",
    "    '''\n",
    "    INPUT\n",
    "    t_price - price with log and min/max transformation \n",
    "    OUTPUT\n",
    "    price in USD.\n",
    "    '''\n",
    "    min_y=min(np.log(mergedDf.price))\n",
    "    max_y=max(np.log(mergedDf.price))\n",
    "\n",
    "    t_price= t_price*(max_y-min_y) + min_y\n",
    "    t_price= np.exp(t_price)\n",
    "\n",
    "\n",
    "    return t_price\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_usd= deform(pred)\n",
    "y_test_usd= deform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = abs(pred_usd - y_test_usd)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2), 'USD.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "ax.scatter(y_test_usd, pred_usd, color=\"turquoise\")\n",
    "plt.xlabel(\"Real\")\n",
    "plt.ylabel(\"Prediction\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = list(rf.feature_importances_)\n",
    "labels = X.columns.tolist()\n",
    "df_imp=pd.DataFrame({'variable': labels,'imp': importances})\n",
    "df_most=df_imp.sort_values(by=['imp'],ascending=False).head(10)\n",
    "\n",
    "#Plot with importances\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(df_most.variable, df_most.imp, 0.35, color='bisque')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "source": [
    "# 5. Results, Evaluation and Insights"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Regarding our initial questions we have the following:\n",
    "\n",
    "1. Is there is a strong correlation between the size of the listing and its price?\n",
    "\n",
    "If we consider the correlation between the number of bathrooms and how many people a listing can accommodate with size, the short answer is yes. \n",
    "\n",
    "2. Is Location the most important variable for demand and pricing?\n",
    "\n",
    "Location (captured by lat and lon) is one of the most important variables to consider when we are taliing about he price of the listing. It is quite obvious locaiton will be important, but its kind of interseting finding that latitude is more important than longitud, meaning that its more relevant deciding wether to invest in real state north/south than east/west.\n",
    "\n",
    "3. Past reviews impact future listings of the place?\n",
    "\n",
    "The actual reviw score did not show up as one of the main variables, althoug the variable 'positive' (wich captures the sentiment of the comments) has some impact on the pricing.\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Wraping up, we found that there is a strong correlation between the location of a listing and its price (kind of obvious), but th real insight here is that the average sentimen of the comments is way more important than the socres given by the guests."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}