{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python395jvsc74a57bd099895b1fb5777b62bbca485c28947babc49174d82e5a9d257e359cdc2b435eee",
   "display_name": "Python 3.9.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 1. Business understanding\n",
    "\n",
    "Assessing payment capacity is one of the most important issues when financial institutions need to assign credit limits. Although it might seem trivial, in some cases the information is no available, and due to the informality of some latinamercian economies (Colombia for this study case), it is important to build statistical models that can estimate the income of the customers. \n",
    "\n",
    "## What data do we have?\n",
    "\n",
    "We are going to be using the data from the National Administrative Department of Statistics of Colombia [DANE] (https://www.dane.gov.co/). The database is the result of a survey conducted to more than 25k households in three major cities in Colombia for 2018. \n",
    "\n",
    "All the data and metadata can be found in [this link] (http://microdatos.dane.gov.co/index.php/catalog/626/). The data has 331 variables including spending behaviours and financial burden of the households.\n",
    "\n",
    "## What question do we want to answer?\n",
    "\n",
    "1. Can the income be modeled after the spending patterns of the household?\n",
    "2. Can the income be modeled after the financial burden of the household?\n",
    "3. Is there a possible way financial institutions can include this informetion in theor models?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 2. Data Understanding"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "##  Environment and data\n",
    "\n",
    "First af all, lets load all needed packages and teh data we will be working with."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./IEFIC_2018.csv', sep=';')\n",
    "print(df.info())\n",
    "df.head()"
   ]
  },
  {
   "source": [
    "Reading the database documentation it can be found that there are some household that do not report the total income, and that have one entry for every member of the house (including children). For this reason we are going to keep only the household head and only the households that report total income."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[df['INGRESO_COMPLETO']==1]\n",
    "df=df[df['P6050']==1]\n",
    "df=df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "df=df.replace(98, np.nan, regex=True)\n",
    "df=df.replace('98', np.nan, regex=True)\n",
    "df=df.replace(99, np.nan, regex=True)\n",
    "df=df.replace('99', np.nan, regex=True)\n",
    "df=df[df['INGTOTOB'].notna()]\n",
    "df=df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "source": [
    "After reviewing the documentation the following variables were selected. There are basicaly 3 types of variables. Spenditure, debts and investments."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep=['P10','INGTOTOB','P2439','P2461','P2168','P2471_4','P2477','P2478_1','P2478_2','P2478_3','P2478_4','P2478_5','P2478_6','P2478_7','P2478_8','P2478_9','P2478_10','P2478_11','P2478_12','P2481_1','P2481_2','P2481_3','P2481_4','P2481_5','P2481_6','P2481_7','P2481_8','P2481_9','P2481_10','P2481_11','P2481_12','P2481_13','P2481_14','P2481_15','P2481_16','P2481_17','P2481_18','P2982','P2983','P2985','P2487','P2502','P2503','P2504','P342','P2540','P2542_3','P2542_4','P2545','P2548','P2560_3','P2560_4','P2602','P2623_3','P2623_4','P2633','P2637_3','P2637_4','P2692','P2772_3','P2772_4','P2695','P2736_3','P2736_4','P2734','P2696_3','P2696_4','P2771','P2693_3','P2693_4','P2819','P2869','P622','P1136','P1239','P1421','P2584','P2962']\n",
    "\n",
    "df=df[keep]\n",
    "\n",
    "names=['edu_level','income','house_owner','house_value','mortage','mortage_balance','spent_edu','spent_food','spent_clothes','spent_water','spent_energy','spent_gas','spent_cell','spent_housekeep','spent_leisure','spent_health','spent_internet','spent_transport','spent_pension','extra_house','extra_home','extra_jewelry','extra_art','extra_rent','extra_vacations','extra_retirement','extra_emergency','extra_future','extra_edu','extra_debts','extra_health','extra_children','extra_wedding','extra_invest','extra_heritage','extra_remodeling','extra_savings','small_business','small_business_value','real_estate','real_estate_value','vehicles','vehicles_value','machinery','machinery_value','credit_cards','credit_cards_payment','credit_cards_balance','credit_cards_term','pawnshop','pawnshop_payment','pawnshop_balance','loans','loans_payment','loans_balance','shark','shark_payment','shark_balance','shop','shop_payment','shop_balance','union','union_payment','union_balance','edu_loan','edu_loan_payment','edu_loan_balance','friend_loan','friend_loan_payment','friend_loan_balance','stocks','stocks_value','funds','funds_value','tdc','tdc_value','savs_acc','savs_acc_value']\n",
    "\n",
    "df.columns=names\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df[['income','spent_edu','spent_food','spent_clothes','spent_water','spent_energy','spent_gas','spent_cell','spent_housekeep','spent_leisure','spent_health','spent_internet','spent_transport','spent_pension']].isnull(), cbar=False)"
   ]
  },
  {
   "source": [
    "### Lets do some EDA over Listings"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Lets beging diferentiationg numeric and categorical variables"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lis.dtypes"
   ]
  },
  {
   "source": [
    "It seems we have some issues with the two variables 'price' and 'security_deposit'. We have to change the type to float but doing some replacements first."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lis['price'] = df_lis['price'].str.replace('$', '')\n",
    "df_lis['price'] = df_lis['price'].str.replace(',', '')\n",
    "df_lis['price']=df_lis['price'].astype('float')\n",
    "\n",
    "df_lis['host_acceptance_rate'] = df_lis['host_acceptance_rate'].str.replace('%', '')\n",
    "df_lis['host_acceptance_rate']=df_lis['host_acceptance_rate'].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we split numeric and categorical variables in two datasets\n",
    "l_num_vars = df_lis[df_lis.select_dtypes(include=['float', 'int']).columns]\n",
    "l_cat_vars = df_lis[df_lis.select_dtypes(include=['object']).columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets plot some correlations\n",
    "sns.heatmap(l_num_vars.corr())\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "There seem to be some obvoius correlations:\n",
    "\n",
    "1. The scores have the biggest correlation between them.\n",
    "2. Between the number of bedrooms and the number of bed.\n",
    "\n",
    "But there are some that are not that simple:\n",
    "\n",
    "1. It seems that the socre that are more correlate with price are location and cleanliness\n",
    "2. Latitude seems to be more correlated with price that longitude"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_lis=l_num_vars.corr().stack().reset_index()\n",
    "corr_lis.columns = ['var_1','var_2','corr']\n",
    "price_corr=corr_lis[corr_lis['var_1']=='price'].sort_values(by=['corr'])  \n",
    "price_corr=price_corr[price_corr['var_2']!='price']\n",
    "\n",
    "x = price_corr['var_2']\n",
    "y = price_corr['corr']\n",
    "\n",
    "mask1 = y < 0\n",
    "mask2 = y >= 0\n",
    "\n",
    "plt.xticks(rotation = 90)\n",
    "plt.bar(x[mask1], y[mask1], color = 'bisque')\n",
    "plt.bar(x[mask2], y[mask2], color = 'turquoise')\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    " Checking correlations only with 'price' it is clear that the number of bedrooms/beds, therefore the size of the listing, is strongly correlated with the price. Location (latitude and longitude) have a some correlation with price."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Reviews!\n",
    "\n",
    "This dataset has unique id for each reviewer and detailed comments\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev=df_rev.drop(['id','date','reviewer_name'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev.groupby(['listing_id']).count().sort_values(by=['comments'],ascending=False).head()"
   ]
  },
  {
   "source": [
    "Some listing have up to 402 comments!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev.groupby(['reviewer_id']).count().sort_values(by=['comments'],ascending=False).head()"
   ]
  },
  {
   "source": [
    "How much traveling does it takes to visit 37 different listing!\n",
    "\n",
    "This dataset is pretty straight forward. In the following section we are going to try and use some sort of prebuilt sentiment analisys tool so we can clasify the comments and try to predict the price :)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 3. Data Preparation\n",
    "\n",
    "In this section we are going to get our databases ready for modeling, doing some data cleaning, transformation and imputation\n",
    "\n",
    "## Reviews!\n",
    "\n",
    "The following list shows the transformation to be made in each of the columns in this dataset\n",
    "\n",
    "**price:** apply logarithm and Normalization \n",
    "\n",
    "1. **host_since:** Calculate the monts this host hase been related to Airbnb and Normalization\n",
    "2 **host_response_time:** Dummify\n",
    "3. **host_response_rate:** Normalization\n",
    "4. **host_acceptance_rate:** Normalization\n",
    "5. **host_is_superhost:** Dummify\n",
    "6. **host_total_listings_count:** Normalization\n",
    "7. **latitude:** Normalization\n",
    "8. **longitude:** Normalization\n",
    "9. **property_type:** Dummify\n",
    "10. **room_type:** Dummify\n",
    "11. **accommodates:** Normalization\n",
    "12. **bathrooms:** Normalization\n",
    "13. **bedrooms:** Normalization\n",
    "14.**beds:** Normalization\n",
    "15.**price:** Normalization\n",
    "16.**guests_included:** Normalization\n",
    "17. **number_of_reviews:** Normalization\n",
    "18. **review_scores_rating:** Normalization\n",
    "19. **review_scores_accuracy:** Normalization\n",
    "20. **review_scores_cleanliness:** Normalization\n",
    "21. **review_scores_checkin:** Normalization\n",
    "22. **review_scores_communication:** Normalization\n",
    "23. **review_scores_location:** Normalization\n",
    "24. **review_scores_value:** Normalization\n",
    "25. **instant_bookable:** Dummify\n",
    "\n",
    "But first, we are going to perform some type changes an calculations in some variables"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we are calculating the number of months between the most recent host and all the others\n",
    "\n",
    "df_lis.host_since=pd.to_datetime(df_lis.host_since)\n",
    "df_lis.host_since=(max(df_lis.host_since)-df_lis.host_since)\n",
    "df_lis.host_since=(df_lis.host_since/ np.timedelta64(1, 'D')).astype(float)/30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we change the type of the percentage.\n",
    "\n",
    "df_lis.host_response_rate = df_lis.host_response_rate.str.replace('%', '')\n",
    "df_lis.host_response_rate = df_lis.host_response_rate.astype(float)"
   ]
  },
  {
   "source": [
    "Before normalizing an getting the dummy variables for the categories, we are going to apply some prebuilt sentiment analyzer on the reviwes. We are going to use NLTK prebuilt sentyment analyzer."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos(comment):\n",
    "    '''\n",
    "    INPUT\n",
    "    comment - string \n",
    "    OUTPUT\n",
    "    Positive score of Nltk sentimen analyzer.\n",
    "    '''\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    pos=sia.polarity_scores(comment).get('pos')\n",
    "    return pos\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we are going to apply our funtion to every coment, and average the value by listing id\n",
    "\n",
    "df_rev.comments = df_rev.comments.astype('str') \n",
    "df_rev['positive']=df_rev.comments.apply(get_pos)\n",
    "df_rev_mean=df_rev.groupby('listing_id')['positive'].mean().to_frame()"
   ]
  },
  {
   "source": [
    "## Joining Resulting Dataframes\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lis=df_lis.set_index('id')\n",
    "mergedDf = df_lis.merge(df_rev_mean, left_index=True, right_index=True)"
   ]
  },
  {
   "source": [
    "This following function is based on the one showd in the solution of some quizes in Udacitys nanodegree.\n",
    "\n",
    "# Something missing?\n",
    "\n",
    "We are going to handle missin values in three ways:\n",
    "\n",
    "1. Price: this is our lable, so we are going to drop all null values.\n",
    "2. Numeric variables: The numeric values are going to be filled with the mean. \n",
    "3. Categorical variables: In this case the absence of variable in the dummies captures the null values.\n",
    "\n",
    "This is safe for our model, becaouse there woulfd be no considerable variance for the inputed variables.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    '''\n",
    "    INPUT\n",
    "    df - pandas dataframe \n",
    "    \n",
    "    OUTPUT\n",
    "    X - A matrix holding all of the variables you want to consider when predicting the response\n",
    "    y - the corresponding response vector\n",
    "    \n",
    "    This function cleans df using the following steps to produce X and y:\n",
    "    1. Drop all the rows with no salaries\n",
    "    2. Create X as all the columns that are not the Salary column\n",
    "    3. Create y as the Salary column\n",
    "    4. Drop the Salary, Respondent, and the ExpectedSalary columns from X\n",
    "    5. For each numeric variable in X, fill the column with the mean value of the column.\n",
    "    5. For each numeric variable in X, perform min max normalization\n",
    "    6. Create dummy columns for all the categorical variables in X, drop the original columns\n",
    "    '''\n",
    "    # Drop rows with missing price values\n",
    "    df = df.dropna(subset=['price'], axis=0)\n",
    "    y = df['price']\n",
    "    y = np.log(y)\n",
    "    y = (y-min(y))/(max(y)-min(y))\n",
    "\n",
    "\n",
    "    #Drop price columns\n",
    "    df = df.drop(['price'], axis=1)\n",
    "    \n",
    "    # Fill numeric columns with the mean\n",
    "    num_vars = df.select_dtypes(include=['float', 'int']).columns\n",
    "    for col in num_vars:\n",
    "        df[col].fillna((df[col].mean()), inplace=True)\n",
    "        df[col]=(df[col]-min(df[col]))/(max(df[col])-min(df[col]))\n",
    "        \n",
    "    # Dummy the categorical variables\n",
    "    cat_vars = df.select_dtypes(include=['object']).copy().columns\n",
    "    for var in  cat_vars:\n",
    "        # for each cat add dummy var, drop original column\n",
    "        df = pd.concat([df.drop(var, axis=1), pd.get_dummies(df[var], prefix=var, prefix_sep='_', drop_first=True)], axis=1)\n",
    "    \n",
    "    X = df\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedDf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = clean_data(mergedDf)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "source": [
    "Now we have the datasets ready to model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 4. Modeling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "In this section we are going to use a simple random forest to predict the price of the listings. We are going be doing the following steps:\n",
    "\n",
    "1. Split data\n",
    "2. Instantiate model and fit\n",
    "4. Evaluate"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Split data\n",
    "\n",
    "The data will bi splitted 80/20 for train and test\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Features Shape:', X_train.shape)\n",
    "print('Training Labels Shape:', y_train.shape)\n",
    "print('Testing Features Shape:', X_test.shape)\n",
    "print('Testing Labels Shape:', y_test.shape)"
   ]
  },
  {
   "source": [
    "## Instantiate and fit model\n",
    "\n",
    "We are going to use a simple Random forest with 1000 estimators.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 321)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "source": [
    "## Evaluate Model\n",
    "\n",
    "Since we did some transformations to the lable (price) for the error meassure to make sense we have to reverse this transormations to our predictions and test labels.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deform(t_price):\n",
    "    '''\n",
    "    INPUT\n",
    "    t_price - price with log and min/max transformation \n",
    "    OUTPUT\n",
    "    price in USD.\n",
    "    '''\n",
    "    min_y=min(np.log(mergedDf.price))\n",
    "    max_y=max(np.log(mergedDf.price))\n",
    "\n",
    "    t_price= t_price*(max_y-min_y) + min_y\n",
    "    t_price= np.exp(t_price)\n",
    "\n",
    "\n",
    "    return t_price\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_usd= deform(pred)\n",
    "y_test_usd= deform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = abs(pred_usd - y_test_usd)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2), 'USD.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "ax.scatter(y_test_usd, pred_usd, color=\"turquoise\")\n",
    "plt.xlabel(\"Real\")\n",
    "plt.ylabel(\"Prediction\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = list(rf.feature_importances_)\n",
    "labels = X.columns.tolist()\n",
    "df_imp=pd.DataFrame({'variable': labels,'imp': importances})\n",
    "df_most=df_imp.sort_values(by=['imp'],ascending=False).head(10)\n",
    "\n",
    "#Plot with importances\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(df_most.variable, df_most.imp, 0.35, color='bisque')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "source": [
    "# 5. Results, Evaluation and Insights"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Regarding our initial questions we have the following:\n",
    "\n",
    "1. Is there is a strong correlation between the size of the listing and its price?\n",
    "\n",
    "If we consider the correlation between the number of bathrooms and how many people a listing can accommodate with size, the short answer is yes. \n",
    "\n",
    "2. Is Location the most important variable for demand and pricing?\n",
    "\n",
    "Location (captured by lat and lon) is one of the most important variables to consider when we are taliing about he price of the listing. It is quite obvious locaiton will be important, but its kind of interseting finding that latitude is more important than longitud, meaning that its more relevant deciding wether to invest in real state north/south than east/west.\n",
    "\n",
    "3. Past reviews impact future listings of the place?\n",
    "\n",
    "The actual reviw score did not show up as one of the main variables, althoug the variable 'positive' (wich captures the sentiment of the comments) has some impact on the pricing.\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Wraping up, we found that there is a strong correlation between the location of a listing and its price (kind of obvious), but th real insight here is that the average sentimen of the comments is way more important than the socres given by the guests."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}